{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import PIL.ImageOps\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import Sequential, Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Add\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connect Line With Session"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "img_files = np.zeros((0), dtype=np.str)\n",
    "img_targets = np.zeros((0), dtype=np.str)\n",
    "directory = \"Lines\"\n",
    "csv = pd.read_csv(\"updatedDataset.csv\")\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    img_files = np.append(img_files, \"Lines/\"+filename)\n",
    "    if len(filename)==17:\n",
    "        i=int(filename[7:8])\n",
    "    elif len(filename)==18:\n",
    "        i=int(filename[7:9])\n",
    "    else:\n",
    "        i=int(filename[7:10])\n",
    "    img_targets = np.append(img_targets, csv['diag'][i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(img_files[0:20], img_targets[0:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(img_targets)\n",
    "encoded_img_targets = encoder.transform(img_targets)\n",
    "\n",
    "print(\"Writer ID        : \", img_targets[:2])\n",
    "print(\"Encoded writer ID: \", encoded_img_targets[:2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CROP_SIZE = 113\n",
    "NUM_LABELS = 2\n",
    "BATCH_SIZE = 16"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Augmentation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def get_augmented_sample(sample, label, sample_ratio):\n",
    "    # Get current image details\n",
    "    img = Image.open(sample).convert('L')\n",
    "    img_width = img.size[0]\n",
    "    img_height = img.size[1]\n",
    "\n",
    "    # Compute resize dimensions such that aspect ratio is maintained\n",
    "    height_fac = CROP_SIZE / img_height\n",
    "    size = (int(img_width * height_fac), CROP_SIZE)\n",
    "\n",
    "    # Resize image\n",
    "    new_img = img.resize(size, Image.ANTIALIAS)\n",
    "    new_img_width = new_img.size[0]\n",
    "    new_img_height = new_img.size[1]\n",
    "\n",
    "    # Generate a random number of crops of size 113x113 from the resized image\n",
    "    x_coord = list(range(0, new_img_width - CROP_SIZE))\n",
    "    num_crops = int(len(x_coord) * sample_ratio)\n",
    "    random_x_coord = random.sample(x_coord, num_crops)\n",
    "\n",
    "    # Create augmented images (cropped forms) and map them to a label (writer)\n",
    "    images = []\n",
    "    labels = []\n",
    "    for x in random_x_coord:\n",
    "        img_crop = new_img.crop((x, 0, x + CROP_SIZE, CROP_SIZE))\n",
    "        # Transform image to an array of numbers\n",
    "        images.append(np.asarray(img_crop))\n",
    "        labels.append(label)\n",
    "\n",
    "    return (images, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import operator\n",
    "from functools import reduce\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def generate_data(samples, labels, batch_size, sample_ratio):\n",
    "    for offset in range(0, len(samples), batch_size):\n",
    "        batch_samples = samples[offset:(offset + batch_size)]\n",
    "        batch_labels = labels[offset:(offset + batch_size)]\n",
    "\n",
    "        # Augment each sample in batch\n",
    "        augmented_batch_samples = []\n",
    "        augmented_batch_labels = []\n",
    "        for i in range(len(batch_samples)):\n",
    "            sample = batch_samples[i]\n",
    "            label = batch_labels[i]\n",
    "            augmented_samples, augmented_labels  = get_augmented_sample(sample, label, sample_ratio)\n",
    "            augmented_batch_samples.append(augmented_samples)\n",
    "            augmented_batch_labels.append(augmented_labels)\n",
    "\n",
    "        # Flatten out samples and labels\n",
    "        augmented_batch_samples = reduce(operator.add, augmented_batch_samples)\n",
    "        augmented_batch_labels = reduce(operator.add, augmented_batch_labels)\n",
    "\n",
    "        # Reshape input format\n",
    "        X_train = np.array(augmented_batch_samples)\n",
    "        print(X_train.shape)\n",
    "        X_train = X_train.reshape(X_train.shape[0], CROP_SIZE, CROP_SIZE, 1)\n",
    "        print(X_train.shape)\n",
    "\n",
    "        # Transform input to float and normalize\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_train /= 255\n",
    "\n",
    "        y_train = np.array(augmented_batch_labels)\n",
    "        y_train = to_categorical(y_train, NUM_LABELS)\n",
    "\n",
    "        return X_train, y_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_generator = generate_data(img_files, encoded_img_targets, BATCH_SIZE, 0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def resize_image(img):\n",
    "    size = round(CROP_SIZE/2)\n",
    "    return tf.image.resize(img, [size, size])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Define network input shape\n",
    "model.add(ZeroPadding2D((1, 1), input_shape=(CROP_SIZE, CROP_SIZE, 1)))\n",
    "# Resize images to allow for easy computation\n",
    "model.add(Lambda(resize_image))\n",
    "\n",
    "# CNN model - Building the model suggested in paper\n",
    "model.add(Convolution2D(filters= 32, kernel_size =(5,5), strides= (2, 2), padding='same', name='conv1'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool1'))\n",
    "\n",
    "model.add(Convolution2D(filters= 64, kernel_size =(3, 3), strides= (1, 1), padding='same', name='conv2'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool2'))\n",
    "\n",
    "model.add(Convolution2D(filters= 128, kernel_size =(3, 3), strides= (1, 1), padding='same', name='conv3'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool3'))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, name='dense1'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256, name='dense2'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50, name='output'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading Weights Of CNN Trained with Dataset IAM and Feature Extraction (from each model we generate 2 csv, one with -6 layer the other -5 layer, changing the value in layers[-6])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#model.load_weights(\"model_checkpoints/check_200.hdf5\")\n",
    "#model.load_weights(\"model_checkpoints/check_20.hdf5\")\n",
    "model.load_weights(\"model_checkpoints/check_100.hdf5\")\n",
    "\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-6].output)\n",
    "\n",
    "trainX, trainY =train_generator\n",
    "\n",
    "\n",
    "for elem in trainX :\n",
    "\n",
    "    features = model.predict(elem)\n",
    "    df1 = pd.DataFrame(features)\n",
    "    df =  pd.concat([df, df1], ignore_index=True)\n",
    "\n",
    "y=list(trainY)\n",
    "y=list(map(int, list(map(lambda x: x[0], y))))\n",
    "res = []\n",
    "for i in y:\n",
    "    for ele in range(113):\n",
    "        res.append(i)\n",
    "df[\"diag\"]=res\n",
    "df.to_csv(\"CNNFeatureExtractionEpoch100.-6layer.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification of the selected FeatureExtraction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "features_df = pd.DataFrame()\n",
    "#features_df = pd.read_csv(\"CNNFeatureExtractionEpoch200.-6layer.csv\")\n",
    "#features_df = pd.read_csv(\"CNNFeatureExtractionEpoch200.-5layer.csv\")\n",
    "#features_df = pd.read_csv(\"CNNFeatureExtractionEpoch20.-6layer.csv\")\n",
    "#features_df = pd.read_csv(\"CNNFeatureExtractionEpoch20.-5layer.csv\")\n",
    "#features_df = pd.read_csv(\"CNNFeatureExtractionEpoch100.-6layer.csv\")\n",
    "features_df = pd.read_csv(\"CNNFeatureExtractionEpoch100.-5layer.csv\")\n",
    "\n",
    "features_df = features_df.drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "train, test = train_test_split(features_df, test_size=20)\n",
    "\n",
    "y = features_df['diag']\n",
    "\n",
    "trainY = train[\"diag\"]\n",
    "trainX = train.drop([\"diag\"], axis=1)\n",
    "\n",
    "testY = test[\"diag\"]\n",
    "testX = test.drop([\"diag\"], axis=1)\n",
    "\n",
    "trainX"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RidgeClassifier(alpha=100)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(model.score(testX,testY))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logModel = LogisticRegression()\n",
    "logModel.fit(trainX, trainY)\n",
    "\n",
    "print(logModel.score(testX,testY))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(trainX, trainY)\n",
    "\n",
    "print(tree.score(testX,testY))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rndforest = RandomForestClassifier(n_estimators=100)\n",
    "rndforest.fit(trainX, trainY)\n",
    "print(rndforest.score(testX,testY))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The accuracy is very variable therefore through this block it is possible to obtain the average results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import statistics\n",
    "accRidge = []\n",
    "accLogReg = []\n",
    "accDecTree = []\n",
    "accRnFr = []\n",
    "ridModel = RidgeClassifier(alpha=100)\n",
    "logModel = LogisticRegression()\n",
    "tree = DecisionTreeClassifier()\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "features_df = pd.read_csv(\"CNNFeatureExtractionEpoch100.csv\")\n",
    "#features_df = pd.read_csv(\"CNNFeatureExtractionEpoch100.csv\")\n",
    "#features_df = pd.read_csv(\"CNNFeatureExtractionEpoch20.csv\")\n",
    "#features_df = pd.read_csv(\"CNNFeatureExtractionEpoch20.csv\")\n",
    "#features_df = pd.read_csv(\"CNNFeatureExtractionEpoch200.csv\")\n",
    "#features_df = pd.read_csv(\"CNNFeatureExtractionEpoch200.csv\")\n",
    "features_df = features_df.drop([\"Unnamed: 0\"], axis=1)\n",
    "for i in range(10):\n",
    "    train, test = train_test_split(features_df, test_size=20)\n",
    "    trainY = train[\"diag\"]\n",
    "    trainX = train.drop([\"diag\"], axis=1)\n",
    "    testY = test[\"diag\"]\n",
    "    testX = test.drop([\"diag\"], axis=1)\n",
    "\n",
    "\n",
    "    ridModel.fit(trainX, trainY)\n",
    "    accRidge.append(ridModel.score(testX, testY))\n",
    "    logModel.fit(trainX, trainY)\n",
    "    accLogReg.append(logModel.score(testX,testY))\n",
    "    tree.fit(trainX, trainY)\n",
    "    accDecTree.append(tree.score(testX,testY))\n",
    "    clf.fit(trainX,trainY)\n",
    "    accRnFr.append(clf.score(testX,testY))\n",
    "\n",
    "print(\"Media Acc RidgeClassifier: \"+str(statistics.mean(accRidge)))\n",
    "print(\"Accuracy piu elevata \"+str(max(accRidge)))\n",
    "print(\"Accuracy piu bassa \"+str(min(accRidge)))\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Media Acc LogisticRegression: \"+str(statistics.mean(accLogReg)))\n",
    "print(\"Accuracy piu elevata \"+str(max(accLogReg)))\n",
    "print(\"Accuracy piu bassa \"+str(min(accLogReg)))\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Media Acc DecisionTreeClassifier: \"+str(statistics.mean(accDecTree)))\n",
    "print(\"Accuracy piu elevata \"+str(max(accDecTree)))\n",
    "print(\"Accuracy piu bassa \"+str(min(accDecTree)))\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Media Acc RandomForestClassifier: \"+str(statistics.mean(accRnFr)))\n",
    "print(\"Accuracy piu elevata \"+str(max(accRnFr)))\n",
    "print(\"Accuracy piu bassa \"+str(min(accRnFr)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}